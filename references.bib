@misc{01_filos2019reinforcementlearningportfoliomanagement,
      title={Reinforcement Learning for Portfolio Management}, 
      author={Angelos Filos},
      year={2019},
      eprint={1909.09571},
      archivePrefix={arXiv},
      primaryClass={q-fin.PM},
      url={https://arxiv.org/abs/1909.09571}, 
}

@misc{02_jiang2017deepreinforcementlearningframework,
      title={A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem}, 
      author={Zhengyao Jiang and Dixing Xu and Jinjun Liang},
      year={2017},
      eprint={1706.10059},
      archivePrefix={arXiv},
      primaryClass={q-fin.CP},
      url={https://arxiv.org/abs/1706.10059}, 
}

@misc{03_Fischer2018Reinforcement,
    address = {N\"{u}rnberg},
    author = {Thomas G. Fischer},
    copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
    keywords = {330; financial markets; reinforcement learning; survey; trading systems; machine learning},
    language = {eng},
    number = {12/2018},
    publisher = {Friedrich-Alexander-Universit\"{a}t Erlangen-N\"{u}rnberg, Institute for Economics},
    title = {Reinforcement learning in financial markets - a survey},
    type = {FAU Discussion Papers in Economics},
    url = {https://hdl.handle.net/10419/183139},
    year = {2018}
}

@Article{04_data4030110,
    AUTHOR = {Meng, Terry Lingze and Khushi, Matloob},
    TITLE = {Reinforcement Learning in Financial Markets},
    JOURNAL = {Data},
    VOLUME = {4},
    YEAR = {2019},
    NUMBER = {3},
    ARTICLE-NUMBER = {110},
    URL = {https://www.mdpi.com/2306-5729/4/3/110},
    ISSN = {2306-5729},
    DOI = {10.3390/data4030110}
}

@article{05_pricope2021deep,
  title={Deep reinforcement learning in quantitative algorithmic trading: A review},
  author={Pricope, Tidor-Vlad},
  journal={arXiv preprint arXiv:2106.00123},
  year={2021}
}

@phdthesis{06_chen2022portfolio,
  title={Portfolio Management: A Deep Reinforcement Learning Approach},
  author={Chen, Yi Yu Ivan},
  year={2022},
  school={Politecnico di Torino}
}

@mastersthesis{07_gullotto2021,
  author    = {Marco Gullotto},
  title     = {Portfolio management and Deep learning: Reinforcement learning and Transformer applied to stock market data},
  school    = {Politecnico di Torino},
  year      = {2021},
  type      = {Trabajo de Fin de MÃ¡ster},
  url       = {https://webthesis.biblio.polito.it/20569/},
  note      = {Consultado: 6 de octubre de 2024}
}


@article{08_sun2023reinforcement,
  title={Reinforcement learning for quantitative trading},
  author={Sun, Shuo and Wang, Rundong and An, Bo},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={14},
  number={3},
  pages={1--29},
  year={2023},
  publisher={ACM New York, NY}
}

@misc{09_benk2022stock,
  title={Stock Trading Using a Deep Reinforcement Learning and Text Analysis},
  author={Benk, Dominik},
  year={2022},
  publisher={Univerzita Karlova, Fakulta soci{\'a}ln{\'\i}ch v{\v{e}}d}
}

@misc{10_li2023surveytransformersreinforcementlearning,
      title={A Survey on Transformers in Reinforcement Learning}, 
      author={Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
      year={2023},
      eprint={2301.03044},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.03044}, 
}

@misc{11_parisotto2019stabilizingtransformersreinforcementlearning,
      title={Stabilizing Transformers for Reinforcement Learning}, 
      author={Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},
      year={2019},
      eprint={1910.06764},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.06764}, 
}

@phdthesis{12_da2022portfolio,
  title={Portfolio Allocation Using Deep Reinforcement Learning: Cryptocurrency Trading as a Markov Decision Process},
  author={da Silva, Jos{\'e} Alexandre Quintela},
  year={2022},
  school={Instituto Politecnico do Porto (Portugal)}
}

@misc{13_alizadeh2021reinforcement,
  title={Reinforcement Learning for Financial Portfolio Management: A study of Neural Networks for Reinforcement Learning on currency exchange market},
  author={Alizadeh, Amin},
  year={2021}
}

@misc{14_kizito2022adaptive,
  title={An Adaptive Strategy For Short-Term Stock Trading Using Reinforcement Learning},
  author={Kizito, Lloyd},
  year={2022}
}

@article{15_zhang2019deep,
  title={Deep reinforcement learning for trading},
  author={Zhang, Zihao and Zohren, Stefan and Roberts, Stephen},
  journal={arXiv preprint arXiv:1911.10107},
  year={2019}
}

@mastersthesis{16_ala2021financial,
  title={Financial portfolio management with evolution strategies-based reinforcement learning},
  author={Ala-Krekola, Wilhelm and others},
  year={2021}
}

@book{17_ahlawat2023reinforcement,
  title={Reinforcement Learning for Finance: Solve Problems in Finance with CNN and RNN Using the TensorFlow Library},
  author={Ahlawat, Samit},
  year={2023},
  publisher={Apress, Berkeley, CA},
  doi={10.1007/978-1-4842-8835-1},
  isbn={978-1-4842-8834-4},
  url={https://doi.org/10.1007/978-1-4842-8835-1}
}


@phdthesis{18_zappavigna2022exploration,
  title={Exploration Techniques for a Deep Reinforcement Learning Trading Agent},
  author={Zappavigna, Andrea},
  year={2022},
  school={Politecnico di Torino}
}

@book{19_portfolio_selection_markowitz,
 ISBN = {9780300013726},
 URL = {http://www.jstor.org/stable/j.ctt1bh4c8h},
 abstract = {Applies modern techniques of analysis and computation to the problem of finding combinations of securities that best meet the needs of the private institutional investor. Written primarily with the nonmathematician in mind, although it contains mathematical development of the subject in appendixes.},
 author = {Harry M. Markowitz},
 publisher = {Yale University Press},
 title = {Portfolio Selection: Efficient Diversification of Investments},
 urldate = {2024-10-14},
 year = {1959}
}
@inproceedings{20_Sharpe1994TheSR,
  title={The Sharpe Ratio},
  author={William F. Sharpe},
  year={1994},
  url={https://api.semanticscholar.org/CorpusID:55394403}
}

@article{22_the_science_behind_risk_management,
    author = {Johnston, Douglas and Djuric, Petar},
    year = {2011},
    month = {10},
    pages = {26 - 36},
    title = {The Science Behind Risk Management},
    volume = {28},
    journal = {Signal Processing Magazine, IEEE},
    doi = {10.1109/MSP.2011.941549}
}

@inbook{23_doi:10.1142/9789814759588_0001,
    author = { Fischer   Black  and  Myron   Scholes },
    title = {The Pricing of Options and Corporate Liabilities},
    booktitle = {World Scientific Reference on Contingent Claims Analysis in Corporate Finance},
    chapter = {},
    pages = {3-21},
    doi = {10.1142/9789814759588_0001},
    URL = {https://www.worldscientific.com/doi/abs/10.1142/9789814759588_0001},
    eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9789814759588_0001},
    abstract = { If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default. }
}

@online{24_investopediaRisk,
  author       = {Investopedia Staff},
  title        = {Risk and Diversification},
  year         = {2024},
  url          = {https://www.investopedia.com/terms/r/risk.asp#toc-what-is-risk},
  note         = {Accessed: 19-Oct-2024}
}

@online{25_libretextsVarianza,
  author       = {Siegrist, Kyle},
  title        = {La varianza de la muestra},
  year         = {2024},
  url          = {https://espanol.libretexts.org/Estadisticas/Teoria_de_Probabilidad/Probabilidad\%2C_estad\%C3\%ADstica_matem\%C3\%A1tica_y_procesos_estoc\%C3\%A1sticos_(Siegrist)/06\%3A_Muestras_aleatorias/6.05\%3A_La_varianza_de_la_muestra},
  note         = {Accessed: 19-Oct-2024}
}

@book{26_winder2021reinforcement,
  title={Reinforcement Learning: Industrial Applications of Intelligent Agents},
  author={Winder, Phil},
  year={2021},
  edition={1st},
  publisher={O'Reilly Media},
  address={Beijing}
}

@book{27_bellman1957dynamic,
  title={Dynamic Programming},
  author={Bellman, Richard},
  year={1957},
  publisher={Princeton University Press},
  address={Princeton, NJ}
}

@article{28_barto1983sequential,
  title={Sequential Decision Problems and Neural Networks},
  author={Barto, Andrew G. and Sutton, Richard S.},
  journal={Proceedings of the IEEE},
  volume={71},
  number={10},
  pages={1545--1558},
  year={1983},
  publisher={IEEE}
}

@phdthesis{29_watkins1989learning,
  title={Learning from Delayed Rewards},
  author={Watkins, Christopher J. C. H.},
  year={1989},
  school={University of Cambridge}
}
@article{30_rummery1994online,
author = {Rummery, G. and Niranjan, Mahesan},
year = {1994},
month = {11},
pages = {},
title = {On-Line Q-Learning Using Connectionist Systems},
journal = {Technical Report CUED/F-INFENG/TR 166}
}

@ARTICLE{31_reinforcement_learning_an_introduction_sutton_barto,
  author={Sutton, R.S. and Barto, A.G.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Reinforcement Learning: An Introduction}, 
  year={1998},
  volume={9},
  number={5},
  pages={1054-1054},
  keywords={Books;Neural networks;Dynamic programming;Machine learning;Learning systems;Artificial intelligence;Artificial neural networks;Bibliographies;Neurofeedback;Function approximation},
  doi={10.1109/TNN.1998.712192}}

@article{32_szepesvari2009reinforcement,
  title={Reinforcement learning algorithms for MDPs},
  author={Szepesv{\'a}ri, Csaba},
  year={2009}
}

@book{33_sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  edition={2},
  year={2018},
  publisher={MIT Press},
  address={Cambridge, MA}
}

@article{34_mihaly_ormos_doi:10.1080/14697688.2011.570368,
author = {MihÃ¡ly Ormos and AndrÃ¡s UrbÃ¡n},
title = {Performance analysis of log-optimal portfolio strategies with transaction costs},
journal = {Quantitative Finance},
volume = {13},
number = {10},
pages = {1587--1597},
year = {2013},
publisher = {Routledge},
doi = {10.1080/14697688.2011.570368},
URL = {    
        https://doi.org/10.1080/14697688.2011.570368
},
eprint = {  
        https://doi.org/10.1080/14697688.2011.570368
}
}

@inproceedings{35_moody1998reinforcement,
  title={Reinforcement Learning for Trading Systems and Portfolios.},
  author={Moody, John E and Saffell, Matthew and Liao, Y and Wu, L},
  booktitle={KDD},
  pages={279--283},
  year={1998}
}

@article{36_deep_reinforcement_learning_aske_plaat,
  author       = {Aske Plaat},
  title        = {Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/2201.02135},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.02135},
  eprinttype    = {arXiv},
  eprint       = {2201.02135},
  timestamp    = {Mon, 10 Jan 2022 13:39:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-02135.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{37_dqn_DBLP:journals/corr/MnihKSGAWR13,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602},
  eprinttype    = {arXiv},
  eprint       = {1312.5602},
  timestamp    = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{38_openai_ppo_DBLP:journals/corr/SchulmanWDRK17,
  author       = {John Schulman and
                  Filip Wolski and
                  Prafulla Dhariwal and
                  Alec Radford and
                  Oleg Klimov},
  title        = {Proximal Policy Optimization Algorithms},
  journal      = {CoRR},
  volume       = {abs/1707.06347},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.06347},
  eprinttype    = {arXiv},
  eprint       = {1707.06347},
  timestamp    = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{39_teorema_gradiente_lehmann2024definitiveguidepolicygradients,
      title={The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations}, 
      author={Matthias Lehmann},
      year={2024},
      eprint={2401.13662},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.13662}, 
}

@article{40_soft_actor_critic_algorithms_and_applications_haarnoja2018soft,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@misc{41_soft_actor_critic_haarnoja2018softactorcriticoffpolicymaximum,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.01290}, 
}

@misc{42_towers2024gymnasiumstandardinterfacereinforcement,
      title={Gymnasium: A Standard Interface for Reinforcement Learning Environments}, 
      author={Mark Towers and Ariel Kwiatkowski and Jordan Terry and John U. Balis and Gianluca De Cola and Tristan Deleu and Manuel GoulÃ£o and Andreas Kallinteris and Markus Krimmel and Arjun KG and Rodrigo Perez-Vicente and Andrea PierrÃ© and Sander Schulhoff and Jun Jet Tai and Hannah Tan and Omar G. Younis},
      year={2024},
      eprint={2407.17032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.17032}, 
}

@article{43_stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{gitrepo_ZenliiPortfolioManagement,
  author       = {Zenlii},
  title        = {Deep Reinforcement Learning for Portfolio Management},
  year         = {2023},
  url          = {https://github.com/Zenlii/Deep-Reinforcement-Learning-for-Portfolio-Management},
  note         = {Accedido: noviembre 14, 2024},
}

@misc{gitrepo_WassnamePortfolioManagement,
  author       = {Michael J. Clark (wassname)},
  title        = {RL Portfolio Management},
  year         = {2017},
  url          = {https://github.com/wassname/rl-portfolio-management},
  note         = {Accedido: noviembre 14, 2024},
}

